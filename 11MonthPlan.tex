\section{Project summary / abstract}

In the past few decades, much research has been done on digital sound synthesis. Using a physical model (PM) of a musical instrument, rather than for example using sampling synthesis for creating sound, makes the playability of the digital instrument very flexible. In other words, the now-virtual instrument can easily adapt to changes in performance and playing style. Many PMs already exist and can output high quality sound which is sometimes indistinguishable from the real instrument.

“Why not just use the real instrument?” you might ask. Physical modelling makes it possible to extend traditional instruments in ways that would be impossible in the real world. Changing the shape, size or material of the instrument over time potentially results in interesting sounds and could even extend the possibilities for expression for the musician. Furthermore, old or rare instruments that can not be played anymore because they might be damaged or because they are museum pieces can be resurrected virtually.

Going a step further, different components of different instruments can be linked together to create hybrid instruments. Most instruments can be divided into an exciter and a resonator component. Virtually connecting, for example, the mouthpiece of a trumpet to the string of a guitar could potentially lead to interesting sounds.

There are various techniques to implement physical models, where there is a tradeoff between accuracy and speed making high-quality physical modelling very computationally expensive. Only recently have we been able to run existing highly accurate PMs in real-time, making them playable for performers and musicians. The PhD project will focus on implementing existing PMs using techniques focusing on accuracy in real-time.

As the implemented models will be playable in real-time, I will investigate the use of different controllers, such as the Sensel Morph \cite{Sensel2019}. and the Haply \cite{Haply2019}.

The goal of this project will be to push the state of the art in real-time accurate physical models and explore the possibilities that come with this, such as changing physically fixed parameters in real-time and combining components of different instruments to allow for novel interactions and create sounds never heard before. To showcase the project, an application will be made where the user can build their own virtual instrument using the physical models created during the course of the PhD. 

\section{The scientific content of the PhD project}

\subsection{Background}
Physical modelling, along with sampling synthesis (also known as wavetable synthesis \cite{Smith2010a}) and spectral modelling, is a technique to synthesise sound. According to many, physical modelling is the best technique to realistically and naturally simulate real-world musical instruments \cite{Valimaki2006, Smith2010b, Bilbao2009}. As this technique simulates the instrument based on its physics rather than using pre-recorded samples, it is more flexible to player-interaction and thus more realistic when synthesising sound in performance. Although physical models (PMs) could potentially sound indistinguishable from the instrument that they are simulating, it has been impossible - until recently - to make high-quality — which I define as non-linear and based on finite-difference schemes (FDSs) — PMs ‘playable’ in real time \cite{Smith2010a}. With the computational power we currently possess, we can run the PMs in real-time and make them available for musicians in latency-less applications.

Even though there are already existing real-time PMs (see Section \ref{SOTA}), to my opinion, they do not make use of the full potential of using physical modelling for sound synthesis. PMs make it possible for parameters like shape, size, material properties, etc. to be changed, which is physically impossible or very hard to do. Furthermore, different instrument components can be combined to create hybrid instruments which can result in unique sounds that can only be created using PMs.

\subsection{State-of-the-art}\label{SOTA}
This section will give a brief introduction on the state of the art on physical models, real-time implementation of these, modular environments for sound synthesis and instrument extension.

\subsubsection*{Physical Modelling}
The physics of musical instruments can be well described by partial differential equations (PDEs) (see \cite{Bilbao2009} among many others). Many of these equations and other knowledge currently available on the physics of musical instruments have been collected by N. H. Fletcher and T. D. Rossing in \cite{Fletcher1998}. In the past decades, much research has been done on implementing these PDEs to model and simulate different musical instruments. Great overviews of implementation techniques are given by, for example, Vesa Välimäki et al. in \cite{Valimaki2006} and Julius O. Smith in \cite{Smith2010a, Smith2010b}. Some techniques that are described in this literature can be found below:
\\
\\
\emph{Modal Synthesis} decomposes a system into a series of uncoupled ‘modes of vibration’. First used in a musical context by Morrison and Adrien in \cite{Morrison1993} it is a technique that is still used today due to its computational efficiency when simulating higher dimensional systems such as plates (2D) or rooms (3D). It is especially effective when used to describe a linear system \cite{Bilbao2018} with a small number of long-resonating modes \cite{Smith2010a}. When used to describe non-linear systems, however, the modes become `coupled’ and the system will quickly become more computationally expensive. 
\\
\\
\emph{Finite-Difference Time Domain} methods (FDTDs) (or Finite-Difference Schemes (FDS)) aim to solve PDEs by approximating them with difference equations, discretising a continuous system into grid-points in space and time. In a musical context, this technique was first used for the case in string vibration in \cite{Ruiz1969, Hiller1971I, Hiller1971II} and later in \cite{Chaigne1992, Chaigne1994}. Stefan Bilbao extensively describes this method in \cite{Bilbao2009, Bilbao2018}. Although very computationally expensive, this technique can most accurately resemble any system, whether it is linear or non-linear, time-invariant or time-variant.
\\
\\
\emph{Digital Waveguide Modelling} (or Digital Waveguides (DWG)) is a modelling technique that discretises wave propagation and scattering. The technique was first presented in \cite{Smith1992}, and is mostly used for one-dimensional systems, such as strings and acoustic tubes and decomposes their system into travelling wave components. This technique has also been used in higher-dimensional systems, but is superior in efficiency when used in the one-dimensional case \cite{Valimaki2006}. Some authors have combined DWGs with FDS (such as in \cite{Erkut2002, Maestre2014}) to accurately model non-linear behaviour while maintaining high-speed implementation.
\\
\\
\textit{Mass-spring networks} can be similar in nature to FDTD methods, but treat each grid point as an individual mass connected to other masses through springs in a network. Pioneered in a musical context by Cadoz in \cite{Cadoz1979, Cadoz1983, Cadoz1993} it is being further developed by mi-creative in a real-time, interactive environment \cite{Villeneuve2019, Leonard2019}. 

\subsubsection*{Real-time implementation}
Even though physical modelling has been a popular research field in the past few decades, relatively little research has been done on making the models work in real-time, i.e. `playable’ \cite{Mehes2016}. Several virtual string instruments and different electric pianos have been made real-time by F. Pfeifle and R. Bader \cite{Pfeifle2012, Pfeifle2015, Pfeifle2017}. They used field programmable gate arrays (FPGA) for implementing models based on FDSs. Furthermore, Roland’s V-series use COSM (Composite Object Sound Modelling) technology \cite{Bybee2019} that implement real-time physical models in hardware instruments.

\subsubsection*{Modular Environments for Sound Synthesis}

As will be described in the next section, the goal of the project is to create a modular environment consisting of different virtual exciters and resonators. An existing modular environment for sound synthesis is the Synth Kit by littleBits \cite{littleBits2019} where the different modules are oscillators, filters, sequencers, etc., essentially making it a modular synthesiser where the components can physically be moved around and connected. Another example is MuX by Decochon \cite{MuX2019}, a modular synthesiser that a user can build and control in virtual reality. Both of these examples present an alternative way to control a modular synthesiser and are not based on physical models.

In the NESS project \cite{NESS2016}, Stefan Bilbao and his team focused on implementing systems using FDTD methods in real-time. In one of the papers on this \cite{Webb2015} they present a real-time physically modelled implementation of strings/bars and plates in a modular environment. Using C combined with AVX and multithreading they are able to make a system that can run a model of a plate with approximately 7000 grid points virtually connected to 10 strings run on a 4-core processor with a base clock rate of 3.1GHz. Their paper will be a good starting point for creating real-time models for musical instruments rather than ‘simple’ 1D and 2D models.

\subsubsection*{Extending Physical Models: From physically static to virtually dynamic}
There is a lot of potential when using physical modelling to make physically fixed parameters virtually dynamic. The main reasons to do this is to create sonically interesting results and, extend the possibilities and (potentially) enhancing expressivity for the musician.
An example of existing dynamic physical models is Michon’s BladeAxe \cite{Michon2016}, which allows the user to alter the string length in real-time with the only control being the speed of an LFO. In \cite{Gelineck2005}, the authors present a flute of which the length can be changed in real-time using a foot-pedal. In \cite{Willemsen2017}, I presented extensions to a plate reverb where the length and width of the plate and the microphone positions could be altered in real-time. In the PhD project extensions like these will be further explored.

\subsection{Project objectives}
Application to allow for dynamic instrument creation (combining different exciters and resonators), parameter changes (needs to have stability and energy analysis) 

This section will state multiple research questions and give some background on each:

\subsubsection*{How can computationally expensive physical models be made playable in real-time?}
A drawback of using physical modelling for sound synthesis is that there is a high computational cost involved causing most existing physical models to not run in real-time. In other words, these virtual instruments are not ‘playable’. One of the focuses of this PhD project will be to implement existing physical models and optimise them so that they work in real-time. 
After having modelled the PMs as precise as possible, I will look at ways to simplify the models while keeping the sound relatively unchanged. Decomposing a PM into several components and taking one of these back into the real world (as mentioned above) will relieve the algorithm of many computations, which is another way to get closer to a real-time application.

\subsubsection*{How can (the sound of) traditional instruments be extended upon...}
This research question can be divided into two parts: 
\\

\noindent\textit{...by making physically fixed parameters of the now-virtual instrument component dynamic? }
\vspace{0.15cm}
\\
\noindent Physical models can go beyond what traditional musical instruments can do. ‘Virtualising’ traditional musical instruments comes with an infinite amount of (physically impossible) possibilities — solid structures can be made flexible, material properties can be changed and limitations of the human body (such as lung capacity for wind instrumentalists, or amount/reach of fingers for guitar players and keyboardists) can be overcome. It would thus be interesting to investigate what parameters of the model could be made dynamic to create sonically interesting outcomes and/or allow musicians for a higher level of expressiveness.

I hypothesise that the most interesting parameters to change are the ones that are correlated to musical qualities such as pitch or timbre. For example, the length of a string is directly correlated to pitch. The radius of a string is too, but a change in this will change the inharmonicity of the string and thus also the timbre.
\\

\noindent\textit{...by combining components of different instruments?}
\vspace{0.15cm}
\\
\noindent Instruments can generally be divided in an exciter and a resonator component \cite{Borin1989}. Examples of exciters are the bow of a violin or the player's lips for a trumpet. Resonators would in this case be the string and body of the violin, or the trumpet itself (the hollow tube). All of this can be modelled and combined to create hybrid instruments. Using the previous examples, we can now make a bowed trumped, or a lip-excited string.
\\

\noindent The ``physical impossibilities" presented in this section could potentially create sounds never heard before and are therefore interesting to explore during the course of the PhD project. 

\subsubsection*{How can the now-virtual instruments be controlled in a natural way?}
A challenge in playing virtual instruments is their control. As with many physical instruments, you interact immediately with the sound-creating object rather than using a keyboard and mouse. Until now, experiments have been done with the Sensel Morph, which is a controller containing ca. 20,000 pressure sensors that allow for highly expressive control of the instruments \cite{Sensel2019}.

Expressivity, however, is not the only thing that makes an instrument interesting and enjoyable to play. The interaction could feel very `dry' as there is no haptic feedback; something which is the case in most physical instruments.  The `feel' can be taken back by a controller allowing for haptic feedback such as the Haply \cite{Haply2019} or `haptuator' \cite{tactile2019}

\subsubsection*{Main objective}
The main objective of the PhD project is to create an application that combines all of the aforementioned research questions: a real-time modular instrument that the user can build himself where the physical parameters of the components can be dynamically changed. As said before, the physics of musical instruments can be well described by PDEs. The application will contain some sort of solver where the user can insert the PDE (or FDS) and have the application analyse and solve it. The analysis stage is important to make sure that the custom PDE will be stable. 

Furthermore, the application will be playable by means of the different controllers mentioned above. The mapping of these will also be made available to the user so that also the interaction is part of the modularity of the application.

\subsection{Key methods}
This section describes the envisioned methods I will use to answer the above research questions and reach the objectives.

How can computationally expensive physical models be made playable in real-time?
The implementation process will be the same for all PMs. The envisioned steps for making the PMs real-time are listed below:

MATLAB
First the PM will be implemented as a prototype in the MATLAB environment. Initially, FDS will be used to implement the instrument(component) with the highest accuracy possible. An energy analysis (as found in [4]) will be carried out to verify energy conservation and confirm correctness of the model. Evaluation techniques from the sonic interaction design community (such as [26]) will be used to determine whether the PM produces a realistic sound.

C++ / JUCE
After correctness of the implementation has been determined, it can now be implemented in real-time. For this, C++ (most likely using the JUCE framework) will be used as this is a very suitable language for real-time implementation [27].

Optimisation and simplification
The possibilities of using GPU, CPU Parallelisation (Multi-threading), Vector Intrinsics (AVX) will be explored to speed up existing algorithms to make them perform in real-time. If after optimisation, the algorithm is still not fast enough, the model will be simplified. A main focus while doing this is to preserve (perceptual) sound quality as much as possible. Parts of the model could also be replaced by computationally more efficient implementation techniques such as using DWGs. The way the PM will be optimised and simplified depends on the model at hand. 

How can physical and virtual instrument components be combined to create sonically interesting outcomes?
In order to create a fully modular environment, all of the components must be implemented in hardware modules rather than a regular computer. Each module will ideally have an input and an output (perhaps implemented in a single cable or connector) but can also only have an input (fx. a speaker module) or an output. For implementing the PMs in hardware modules, the BELA [28], which is specially made for “ultra-low latency” audio applications, will be used.

Examples of physical exciters are a tensioned string, a wooden / metal bar, a trumpet mouthpiece (with breath sensor) or a drum membrane. The sound will be picked up by a contact microphone, electromagnetic pickup (for a string), or a different sensor, depending on the exciter. Examples of physical resonators could be a guitar body, metal plate, but also a string or drum membrane. The resonator will be actuated using vibrating components such as the Tactile Labs’ ‘haptuator’ [29]. 

Experiments will then be carried out on different combinations of modules to determine how natural and realistic the resulting sound is. In later stages of the project, experiments may be carried out to test playability (how well the user can control the HAVI) of different combinations and the interaction of the modular HAVI in general. 

How can (the sound of) traditional instruments be extended upon by making physically fixed parameters of the now-virtual instrument component dynamic? 
When simulating a traditional musical instrument using PMs, it relieves all physical restrictions on the instrument. In terms of a PM, we can say that (almost) all parameters are free or can be made dynamic. Controls will be added to the virtual modules to control these extra parameters. These could be knobs or sliders, but depending on the parameter they could be made more intuitive. If, for example, a virtual model of a trumpet allows for stretching the body of instrument, an actual stretch sensor could be used to control this parameter.

\subsection{Significance and outcome}

\section{Work and publication plans}
\subsection{Work and Time Plans}

\footnotesize
\def \tabcolwidth {0.035\columnwidth}
\begin{tabular}{|c|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|p{\tabcolwidth}|}
\hline
      \bf Year & \multicolumn{2}{c|}{\bf 2018} & \multicolumn{4}{c|}{\bf 2019} &\multicolumn{4}{c|}{\bf 2020}& \multicolumn{2}{c|}{\bf 2021} \\
     \hline
     \centering\bf Quarter & \centering\bf 3 & \centering\bf 4 & \centering\bf 1 & \centering\bf 2 & \centering\bf 3 & \centering\bf 4 & \centering\bf 1 & \centering\bf 2 & \centering\bf 3 & \centering\bf 4 & \centering\bf 1 & \multicolumn{1}{c|}{\bf 2}  \\
     \hline
     Literature study & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & & & & & & & &  \cellcolor{lighterblue}\\
     \hline
     Implementing existing physical & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & & & & & & & &  \cellcolor{lighterblue}\\
     models (using literature) & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & & & & & & & &  \cellcolor{lighterblue}\\
     \hline
     Real-time implementation & & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & & & & & & & &  \cellcolor{lighterblue}\\
     \hline
    Build modular PM & & & & & & & & & & & &  \cellcolor{lighterblue}\\
    framework  & & & & & & & & & & & &  \cellcolor{lighterblue}\\
     \hline
     & & & & & & & & & & & & \cellcolor{lighterblue} \\
     \hline 
     Writing thesis & & & & & & & & &\cellcolor{red} &\cellcolor{red} & \cellcolor{red} &\cellcolor{lighterblue}
     \\
     \hline PhD Courses & \cellcolor{green} & \cellcolor{green} & &  \cellcolor{green} & & & & & & & & \cellcolor{lighterblue}
     \\
     \hline 
      Writing conference /  & & \cellcolor{green} &\cellcolor{green}  & \cellcolor{green} & & & & & & & &
      \cellcolor{lighterblue}\\
     journal paper & & \cellcolor{green} & \cellcolor{green} & \cellcolor{green} & & & & & & & &
      \cellcolor{lighterblue}\\
     \hline 
     Publishing papers & & & \cellcolor{green}\centering C1 & \cellcolor{green}\centering C2 & & & & & & & & \cellcolor{lighterblue}
     \\
     \hline
     Milestones& & \centering MS1 &\centering MS2 & & & & & & & & & \cellcolor{lighterblue}
     \\
     \hline
\end{tabular}
\vspace{0.3cm}
\\
\begin{tabular}[h]{|c|c|c|c|}
\hline
\cellcolor{green} Activities finished & \cellcolor{yellow} Activities being performed & \cellcolor{red} Planned activities & \cellcolor{lighterblue} Buffer time\\
\hline
\end{tabular}
\vspace{0.3cm}
\\
\normalsize
Milestones:
\\
MS1 (Nov): Implement FDS Non-linear Bowed-String model (proving understanding of FDSs).\\
MS2 (Jan): Make bow-model work in real-time using C++/JUCE.\\
MS3 (Apr): Build a physical exciter.\\
MS4 (Jul): Connect physical exciter to the PM effectively creating the first HAVI.\\
MS5 (Apr): Have built multiple physical/virtual exciters/resonators that can interchangeably be connected together.

\subsection{Outline of thesis}
\subsection{Tentative publication list}
C1 \cite{Willemsen2019a}: S. Willemsen, N. Andersson, S. Serafin and S. Bilbao, ``Real-time control of large scale physical models using the Sensel Morph," \textit{Proceedings of the 16th Sound and Music Computing Conference (SMC)}, pp. 275-280, 2019.
\\
C2 \cite{Willemsen2019b}: S. Willemsen, S. Bilbao and S. Serafin, ``Real-time implementation of an elasto-plastic friction model applied to stiff strings using finite difference schemes," \textit{Proceedings of the 22nd International Conference on Digital Audio Effects (DAFx)}, 2019.
\\
C3. S. Willemsen, S. Bilbao and S. Serafin, ``Necessary and sufficient conditions for the elasto plastic friction model," \textit{SOMEWHERE}, 2019.
J1: S. Willemsen, M. Ducceschi, S. Bilbao and S. Serafin, ``Modelling the Tromba Marina: non-linearities using non-iterative methods,"\textit{Computer Music Journal}, 2020.
\\
\section{Supervisor/student co-operation agreements}

\section{Plan for PhD Courses (both general and project related courses)}

\begin{tabular}{|c|c|c|c|c|}
\hline
   Courses  & Place/organised by & ECTS & General / & Status \\
   & & & Project Course & \\
   \hline
   
\rowcolor{lightestblue} Introduction to the PhD Study & AAU / Lars & 0.5 & General &  Finished \\
\rowcolor{lightestblue} & Haastrup Pedersen & & & \\
   Writing and Reviewing & AAU / Jakob Stoustrup & 3.75 & General & Finished \\
   Scientific Papers & & & & \\
\rowcolor{lightestblue} Design and Analysis& AAU / Esben H\o g & 4.5 & General & Finished\\
\rowcolor{lightestblue} of Experiments & & & & \\
   Virtual, Augemented, & AAU / Stefania Serafin & 5 & Project & Enrolled\\
    Mixed Realities & & & &\\
    \hline
\end{tabular}

\section{Plan for fulfilment of knowledge dissemination}
The work done during the course of the project will be presented through conference and journal papers. The conferences and journals that I plan to publish at includes, but are not limited to:
\begin{itemize}
\item The Sound and Music Computing (SMC) Conference
\item The International Conference on Digital Audio Effects (DAFx)
\item The International Conference on New Interfaces for Musical Expression (NIME)
\item Audio Engineering Society (AES) Conferences / Journal
\item Computer Music Journal
\item Journal for New Music Research
\end{itemize}
Furthermore, I will teach (assist) the Physical Models for Sound Synthesis course (SMC8), the Sound Processing course (SMC7) and possibly the Real-time Interaction and Performance and Sound and Music Signal Analysis courses. Lastly, I will supervise group projects in the Sound and Music Computing studies.

\section{Agreements on immaterial rights to patents}
Immaterial rights to patents will follow the standard university rules.

\section{External co-operation}
From the start of the PhD project,  I have collaborated to a great extent with Stefan Bilbao and his team at the University of Edinburgh. I travelled to Edinburgh for two weeks to learn different analysis techniques related to FDSs and plan to do this again during the course of the PhD. Moreover, both past and future papers have/will be in collaboration with him and his team.

Next to this, I will collaborate with Karolina Prawda, a PhD student part of Vesa V\"alim\"aki's team at Aalto University Finland, through the NordicSMC project.

\section{Financing budget}
The PhD project is funded through a PhD Stipend at the Department of Architecture, Design Media Technology (no. 7-18017).
